---
title: "Random Walk Metropolis-Hastings algorithm"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

### **EXAMPLE 1 - "simple" Metropolis-Hastings algorithm**

> We build a random Walk Metropolis-Hastings chain with invariant
> distribution = $\mathcal N(0,1)$.

install.packages("LearnBayes")
library(LearnBayes)

Proposal density: $q(x,y)=f(y-x)$, where
$f(u)=\frac{1}{2\alpha}I_{(-\alpha,\alpha)}(u)$, i.e.
$Y= x+U(-\alpha,\alpha)$.

The state space is $E=\mathbb R$, an open connected set, and $f(u)>0$ in
a neighbour of 0.

We'll run the algorithm under different values for $\alpha$, e.g.
$\alpha=1,10,100$.

The chain starts at 0.

Tipically, for large values of $\alpha$, the acceptance ratio will be
small, i.e. I'll reject the candidate point often, since I'm sampling
the candidate point from a region where the exact posterior density is
very low

```{r}
norm<-function (n, alpha) 
{
        vec <- vector("numeric", n+1)
        x = 0
#        x <- -2*alpha  # RUN the algorithm by yourself with this value as the initial point!
        accept=0
        vec[1] <- x
        for (i in 2:n) {
                innov <- runif(1, -alpha, alpha)
                y <- x + innov
                aprob <- min(1, dnorm(y)/dnorm(x))
                u <- runif(1)
# If condition "(u < aprob)" is NOT met, we'll skip command "x <- y", 
#  so that the MC does not move from x 
                if (u < aprob) 
                     {
                        x <- y
                        accept1=accept+1
                        accept=accept1
                     }
                vec[i] <- x
        }

        vec[n+1]= accept/n
        
        vec
}

```

> $\alpha=1$

Number of iterations of the MC is $k=10,000$

```{r}
k = 10000 # Number of iterations of the MC

normvec<-norm(k,1)
normvec1 = normvec[1:k]
rate1 = normvec[(k+1)]

par(mfrow=c(1,2))
plot(ts(normvec1))
hist(normvec1,30, prob=T)
val=seq(-3,3,0.1)
points(val,dnorm(val),type='l',col='red')


```

Usually we get rid of the first burnin iterations, where the chain
hasn't reached stationarity yet

The Ergodic Theorem says we can use all iteration from any initial point
$x$. However, we are averaging some function of some finite number of
samples, so that out average will be a better approximation if we start
at a typical point in the density we are sampling from and if this
density is closer to the target distribution. We fix burnin=1,000

```{r}
burnin=1000 

b1=burnin+1
normveclast1=normvec[b1:k]

par(mfrow=c(1,1))
plot(density(normveclast1))
points(val,dnorm(val),type='l',col='red')

rate1
```

------------------------------------------------------------------------

> $\alpha=10$

Number of iterations of the MC is $k=10,000$

```{r}
normvec<-norm(k,10)
normvec10 = normvec[1:k]
rate10 = normvec[(k+1)]

par(mfrow=c(1,2))
plot(ts(normvec10))
hist(normvec10,30,prob=T)
points(val,dnorm(val),type='l',col='red')

rate10
```

------------------------------------------------------------------------

> $\alpha=100$

Number of iterations of the MC is $k=10,000$

```{r}
normvec<-norm(k,100)
normvec100 = normvec[1:k]
rate100 = normvec[(k+1)]
par(mfrow=c(1,2))
plot(ts(normvec100))
hist(normvec100,30,prob=T)
points(val,dnorm(val),type='l',col='red')

rate100
```

------------------------------------------------------------------------

COMPARISON among kernel density plots of the simulated MCs for different
values of $\alpha$

```{r}
par(mfrow=c(1,3))
plot(density(normvec1),main="alpha=1")
points(val,dnorm(val),type='l',col='red')
plot(density(normvec10),main="alpha=10")
points(val,dnorm(val),type='l',col='red')
plot(density(normvec100),main="alpha=100")
points(val,dnorm(val),type='l',col='red')
```

COMPARISON among traceplots of the simulated MCs for different values of
$\alpha$

```{r}
par(mfrow=c(1,3))
plot(ts(normvec1[1000:1500]))
plot(ts(normvec10[1000:1500]))
plot(ts(normvec100[1000:1500]))

```

COMPARISON among acceptance rates of the simulated MCs for different
values of $\alpha$

```{r}
rate1;rate10;rate100
```

------------------------------------------------------------------------

------------------------------------------------------------------------

### **EXAMPLE 2 - Ex in 6.7 in Albert (2009)**

Data are heights (in inches) of male students from a local college

The interest is in making inference about the mean and sd of the
heights; $Y_i \sim {\mathcal N}(\mu,\sigma^2)$ iid

1 inch= 2.54 cm

However data are grouped: out of $n=211$ men, $n_1=14$ of them have
height less than 66, $n_2=30$ have height between 66 and 68, ect.

Heights:$<66$, $[66, 68)$, $[68, 70)$, $[70, 72)$, $[70, 72)$,
$[72, 74)$, $\geq 74$

Frequency: $n_1=14$, $n_2=30$, $n_3=49$, $n_4=70$, $n_5=33$, $n_6=15$

d is a list defining the intervals and the frequencies

```{r}
rm(list=ls())
library(LearnBayes)

d=list(int.lo=c(-Inf,seq(66,74,by=2)),int.hi=c(seq(66,74,by=2), Inf),f=c(14,30,49,70,33,15))
d
```


help(groupeddatapost)

it computes the log posterior density of $(\mu,\log \sigma)$ for this
model, i.e., for normal sampling where the data is observed in grouped
form

> We assume an **IMPROPER prior** for $(\mu,\log(\sigma))$, but only because
Jim Albert does: $\pi(\mu,\lambda:=\log(\sigma)) \propto const$ for any $\mu,\lambda$ both in $\mathbb R$ - NEVER assume an improper prior with real data

> **Likelihood** $L(\mu,\sigma;n_1,\ldots,n_6)$: multinomial, with the parameters $(p_1,\ldots,p_6)$, and
$p_i$ is the mass assigned by ${\mathcal N}(\mu,\sigma^2)$ to the $i$-th
class

> **posterior**: $\pi(\mu,\lambda|n_1,\ldots,n_6)\propto L(\mu,e^{\lambda};n_1,\ldots,n_6)$


help(groupeddatapost)

INPUT: $\theta$=values of $\mu$ and $\log(\sigma)$ where we compute the
log-posterior data= dataframe dei dati in forma raggruppata

OUTPUT: log-posterior density


help(laplace)

It is an iterative method (Newton's method) to compute, for a general
posterior density, an estimate of the posterior mode and the associated
variance-covariance matrix

INPUT: the log-posterior density, the initial point of the iterative
method parameters of the log-posterior density (the vector d in this
case)

```{r}
start=c(70,1) #starting point 

fit=laplace(groupeddatapost,start,d)

fit
```

fit\$var is the matrix $(\tilde I_n)^{-1}$, where $\tilde I_n$ is the
"generalized observed Fisher information matrix"\
(-d\^2/ d d log(pi(mu,log(sigma)\|x)) #d is here the DERIVATIVE

```{r}
diag(fit$var)
modal.sds=sqrt(diag(fit$var))
```

Approximately the marginal posterior density of mu is N(70.17,0.035) and
the marginal posterior density of lambda=log(sigma) is N(0.974, 0.003)

#### METROPOLIS-HASTINGS ALGORITHM

Build the MC according to a RANDOM WALK Metropolis algorithm: the
bivariate proposal density $q$ is $\mathcal N$(val_prec,$c^2 V$) where
the scale factor $c=2$, $V$ is the covariance matrix
$(\tilde I_n)^{-1}$; this is the covariance of the Gaussian
approximation

**The PROPOSAL distribution has to be close to the true posterior, i.e.
it should be an approximation of the posterior!!!**

**The scale factor** $c$ determines the behaviour of the MC

> $c=2$

```{r}
proposal=list(var=fit$var,scale=2) #c=2, 0.1, 10
proposal
```

Proposal distributions: \* ${\mathcal N}_2(0,\Sigma)$, \* Uniform on the
ball of radius R, \* t-Student (or more general mixtures of normals)

usually, variance matrix of the Gaussian proposal is $c^2 \cdot$ inverse
of the "generalized observed Fisher information" matrix for c \>0;

here $c =1/2, 1, 2$ work "well"

The function/command of LearnBayes package is rwmetrop

It simulates iterates of a random walk Metropolis chain for an arbitrary
real-valued posterior density defined by the user

help(rwmetrop)

print(rwmetrop)

INPUT: log-posterior = groupeddatapost proposal = N(0,c\^2\*V) start=
initial point of the MC number of iterations=10000 d=parametri della
log-posterior = Hyperparameters + data

the acceptance probability $\alpha(x,y)$ is the ratio of the posterior
densities (random walk MH)

```{r}
fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
```

OUTPUT :

par = a matrix of simulated values where each row corresponds to a
simulated value of the vector parameter

accept= the acceptance rate of the algorithm

```{r}
#fit2
fit2$accept
```

> Is the acceptance rate good?

The proposal should be more "diffuse" than posterior density in order to
explore the whole support of the posterior density.

If $c$ is too small, the acceptance rate will be high; basically, a
small scale $c$ means that I'm sampling the candidate point $y$ from a
density with small variance, so that $y$ will be close to $x$, and
therefore $\alpha(x,y)=\frac{\pi(y)}{\pi(x)}$ will assume values close
to 1.

This means that the MC explores the state space weakly, i.e. the MC
moves slowly through the state space, and the algorithm can turn out to
be inefficient.

On the other hand, if $c$ is too large, $\alpha(x,y)$ will often be
small, and many candidate points will be rejected, so that the overall
acceptance rate $r$ will be too small, and also in this case the
algorithm is inefficient.

A "good" acceptance rate for RW-MH shold be in $[0.2, 0.5)$ for
dimension =1,2,3

For a random walk MH, remember that $\alpha(x,y)=\frac{\pi(y)}{\pi(x)}$
indicates how probable the new proposed sample $y$ is with respect to
the current sample $x$.

If we attempt to move to a point that is more probable than the existing
point (i.e. a point in a higher-density region of $\pi$), we will always
accept the move. However, if we attempt to move to a less probable
point, we will sometimes reject the move, and the more the relative drop
in probability, the more likely we are to reject the new point.

For **independence chain Metropolis**, rates of acceptance that lead to
efficient algorithms have to be higher; theoretically, the higher the
ratio of acceptance, the better the convergence rate of this chain.

Let's go back to posterior inference:

Posterior means and standard deviations (i.e. ergodic means) of $\mu$
and $\log(\sigma)$:

```{r}
post.means=apply(fit2$par,2,mean)
post.sds=apply(fit2$par,2,sd)
post.means
post.sds
```

Comparison between ergodic means and standard deviations and those from
the Gaussian approximation of the posterior

```{r}
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
```

In this case the approximation is very good

Draw a countour plot of the true posterior (which is known beyond the
normalizing constant) with the MC points (from 5001-th on)

```{r}
mycontour(groupeddatapost,c(69,71,.6,1.3),d)
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
```

Marginal traceplots

```{r}
par(mfrow=c(1,2))
plot(ts(fit2$par[5001:10000,1]),ylab="mu")
plot(ts(fit2$par[5001:10000,2]),ylab="sigma")
```

------------------------------------------------------------------------

> $c=0.1$

```{r}
proposal=list(var=fit$var,scale=0.1) 
proposal

fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
fit2$accept
```

```{r}
post.means=apply(fit2$par,2,mean)
post.sds=apply(fit2$par,2,sd)
post.means
post.sds
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
```

```{r}
mycontour(groupeddatapost,c(69,71,.6,1.3),d)
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
```

```{r}
par(mfrow=c(1,2))
plot(ts(fit2$par[5001:10000,1]),ylab="mu")
plot(ts(fit2$par[5001:10000,2]),ylab="sigma")
```

------------------------------------------------------------------------

> $c=10$

```{r}
proposal=list(var=fit$var,scale=10) 
proposal

fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
fit2$accept
```

```{r}
post.means=apply(fit2$par,2,mean)
post.sds=apply(fit2$par,2,sd)
post.means
post.sds
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
```

```{r}
mycontour(groupeddatapost,c(69,71,.6,1.3),d)
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
```

```{r}
par(mfrow=c(1,2))
plot(ts(fit2$par[5001:10000,1]),ylab="mu")
plot(ts(fit2$par[5001:10000,2]),ylab="sigma")
```
