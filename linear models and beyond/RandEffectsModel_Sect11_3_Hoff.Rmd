---
title: "RandomEffectsModels"
output: html_notebook
---

### Example from Hoff's textbook, Sections 11.1 and 11.3

```{r}
 
### I will need these funtions later
### log-density of the multivariate normal distribution
ldmvnorm<-function(X,mu,Sigma,iSigma=solve(Sigma),dSigma=det(Sigma)) 
{
  Y<-t( t(X)-mu)
  sum(diag(-.5*t(Y)%*%Y%*%iSigma))  -
  .5*(  prod(dim(X))*log(2*pi) +     dim(X)[1]*log(dSigma) )
                                 
}
###


### sample from the multivariate normal distribution
rmvnorm<-function(n,mu,Sigma)
{
  p<-length(mu)
  res<-matrix(0,nrow=n,ncol=p)
  if( n>0 & p>0 )
  {
    E<-matrix(rnorm(n*p),n,p)
    res<-t(  t(E%*%chol(Sigma)) +c(mu))
  }
  res
}
###


### sample from the Wishart distribution
rwish<-function(n,nu0,S0)
{
  sS0 <- chol(S0)
  S<-array( dim=c( dim(S0),n ) )
  for(i in 1:n)
  {
     Z <- matrix(rnorm(nu0 * dim(S0)[1]), nu0, dim(S0)[1]) %*% sS0
     S[,,i]<- t(Z)%*%Z
  }
  S[,,1:n]
}
```

### DATA NESTED in GROUPS (hierarchy), LMM (linear mixed-effects model)

#### The school, the mathscore (response), the SES (covariate) of the student

##### Data from 10-th grade children from 100 different large urban public high schools for a total of 1993 students

```{r}
setwd("/Users/xueyuanhuang/Desktop/R-bayesain/linear models and beyond")
Y=read.table("school.mathscore.txt")
head(Y)
```

stu_ses is an indicator of the student's family socioeconomic status

Per group: empirical means and empirical variances of the mathscore, sample sizes

```{r}
m = length(unique(Y[,1]))
n<-sv<-ybar<-rep(NA,m) 
for(j in 1:m) 
{ 
  ybar[j]<- mean(Y[Y[,1]==j,2])   #mean(Y[[j]])
  sv[j]<-var(Y[Y[,1]==j,2])
  n[j]<-sum(Y[,1]==j)
}
```

#### Covariates

X is a list of m=100 matrices:

-   each matrix has a number of rows = number of students in the school (included in the sample)

-   each matrix has a number of columns = 2: the first column contains 1's, the second contains the value of covariate SES (CENTRED wrt the group mean) of each student in the school

```{r}
X<-list() ; 
for(j in 1:m) 
{
  xj<-Y[,3][Y[,1]==j] #Y[,1] contains the index of the group/school  
  xj<-(xj-mean(xj))
  X[[j]]<-cbind( rep(1,n[j]), xj  )
}
```

Summary of the whole empirical distribution of SES (centered - mean=0)

```{r}
ses_cen=Y[,3]
summary(ses_cen)
```

Scatteplot of the data (mathscores and SESs)

```{r}
plot(Y[,3],Y[,2],xlab='ses',ylab='mathscore')
```

#### Least Squares Estimates within each group (j=1,...,100)

We assume that relationship between SES and mathscore depends on the school

```{r}
S2.LS<-BETA.LS<-NULL
for(j in 1:m) {
  fit<-lm(Y[Y[,1]==j,2] ~ -1+X[[j]] ) # we remove the intercept from lm, already in X
  BETA.LS<-rbind(BETA.LS,c(fit$coef)) 
  S2.LS<-c(S2.LS, summary(fit)$sigma^2) 
                } 
```

```{r}
par(mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
par(mfrow=c(1,3))
#LEFT panel: 100 different LS regression lines AND the average of 
#    these lines (the incercept is the average of all 100 estimated MLE intercepts) 
#    - slope similarly)
plot( range(ses_cen),range(Y[,2]),type="n",xlab="SES", 
   ylab="math score")
for(j in 1:m) {    abline(BETA.LS[j,1],BETA.LS[j,2],col="gray")  }

BETA.MLS<-apply(BETA.LS,2,mean)
abline(BETA.MLS[1],BETA.MLS[2],lwd=2)

# MIDDLE panel
plot(n,BETA.LS[,1],xlab="sample size",ylab="intercept") 
 # intercepts of the different regression lines vs group (school) sizes
abline(h= BETA.MLS[1],col="black",lwd=2)

# RIGHT panel
plot(n,BETA.LS[,2],xlab="sample size",ylab="slope") 
#slopes of the different regression lines vs group (school) sizes
abline(h= BETA.MLS[2],col="black",lwd=2)
```

BLACK lines represent AVERAGE values (average line, average intercept, average slope)

Schools with the highest sample sizes have regression coefficients that are generally close to the average, whereas schools with extreme coefficients are generally those with low sample sizes.

> Bayesian solution: stabilize the estimates for small sample size schools by SHARING INFORMATION ACROSS GROUPS, using a hierarchical model

Many regression lines show a positive slope, pointing out that, for many groups, as SES increases, mathscore increases as well.

However, there are more than 15 schools with negative slope!

```{r}
sum(BETA.LS[,2]<0)
```

### HIERARCHICAL REGRESSION MODEL - LMM LINEAR MIXED effects MODEL

#### Prior hyperparameters

$\mu_0$, the prior expectation of $\theta$ is fixed equal to the average of the corresponding (frequentist) regression parameters

The matrix Lambda0 is the empirical covariance matrix of these 100 estimates

```{r}
p<-dim(X[[1]])[2]
theta<-mu0<-apply(BETA.LS,2,mean)
nu0<-1 ; s2<-s20<-mean(S2.LS)
eta0<-p+2 ; #so that the prior of the matrix Sigma is diffuse
L0=matrix(nrow=2,ncol=2)
L0[1,1]=cov(BETA.LS)[1,1]
L0[1,2]=cov(BETA.LS)[1,2]
L0[2,1]=cov(BETA.LS)[2,1]
L0[2,2]=cov(BETA.LS)[2,2]
```

Initial point of the chain

```{r}
Sigma<-S0<-L0 #<-as.matrix(cov(BETA.LS)); 
BETA<-BETA.LS
THETA.b<-S2.b<-NULL
iL0<-solve(L0) ; iSigma<-solve(Sigma)
Sigma.ps<-matrix(0,p,p)
SIGMA.PS<-NULL
BETA.ps<-BETA*0
BETA.pp<-NULL
set.seed(1)
```

#### Parameters: $\beta_1$,...,$\beta_m$, with $m=100$, $\theta$, SIGMA, $\sigma^2$

#### dim($\beta_i$)=dim($\theta$)=2, SIGMA 2-by-2 matrix

$\sigma ^2$ = variance of each response variable (we assume it constant to simplify), its marginal prior is inv-gamma($\nu_0/2,\sigma_0^2 \nu_0/2$), with $\nu_0=1$ and $\sigma_0^2$= average value of the empirical variances per group

$\beta_1,...,\beta_m |\theta$,SIGMA iid $N_2(\theta,SIGMA)$ and $\theta \sim N_2(\mu_0,L_0)$, with $\mu_0$= vector of the means (per group) of the LSE estimates of the intercepts and slopes

$L_0$ is the matrix of empirical covariances, per group, of the LSE of intercepts and slopes

SIGMA ~ inv-Wishart($\eta_0,S_0^{-1}$), with $\eta_0=4$, so that $E(SIGMA)=1/(\eta_0-p-1)* S_0= S_0$ fixed as equal to empirical covariances of the LSE

Gibbs Sampler cycle

```{r}
for(s in 1:10000) {  #10000 iterations - 5 min long
  ##update beta_j 
  for(j in 1:m) 
  {  
    Vj<-solve( iSigma + t(X[[j]])%*%X[[j]]/s2 )
    Ej<-Vj%*%( iSigma%*%theta + t(X[[j]])%*%Y[Y[,1]==j,2]/s2 )
    BETA[j,]<-rmvnorm(1,Ej,Vj) 
  } 
  ##

  ##update theta
  Lm<-  solve( iL0 +  m*iSigma )
  mum<- Lm%*%( iL0%*%mu0 + iSigma%*%apply(BETA,2,sum))
  theta<-t(rmvnorm(1,mum,Lm))
  ##

  ##update Sigma
  mtheta<-matrix(theta,m,p,byrow=TRUE)
  iSigma<-rwish(1, eta0+m, solve( S0+t(BETA-mtheta)%*%(BETA-mtheta) )  ) 
  ##

  ##update s2
  RSS<-0
  for(j in 1:m) { RSS<-RSS+sum( (Y[Y[,1]==j,2]-X[[j]]%*%BETA[j,] )^2 ) }
  s2<-1/rgamma(1,(nu0+sum(n))/2, (nu0*s20+RSS)/2 )
  ##
  ##store results
  if(s%%10==0) 
  { 
    cat(s,s2,"\n")
    S2.b<-c(S2.b,s2);THETA.b<-rbind(THETA.b,t(theta))
    Sigma.ps<-Sigma.ps+solve(iSigma) ; BETA.ps<-BETA.ps+BETA
    SIGMA.PS<-rbind(SIGMA.PS,c(solve(iSigma)))
    BETA.pp<-rbind(BETA.pp,rmvnorm(1,theta,solve(iSigma)) )
  }
  ##
}
## END of the CYCLE
```

thinning=10, so that the final sample size is 1000 - the run lasts more than 5 minutes

```{r}
#save.image("data.f11-3")
#load("data.f11-3")
```

##### Convergence diagnostics

```{r}
library(coda)
effectiveSize(S2.b)        # sigma^2
effectiveSize(THETA.b[,1]) #theta_1
effectiveSize(THETA.b[,2])  #theta_2

apply(SIGMA.PS,2,effectiveSize)  #\Sigma
```

```{r}
par(mfrow=c(2,2))
tmp<-NULL;for(j in 1:dim(SIGMA.PS)[2]) { tmp<-c(tmp,acf(SIGMA.PS[,j])$acf[2]) }

acf(S2.b)
acf(THETA.b[,1])
acf(THETA.b[,2])
```

##### Plot of the marginal posterior of $\theta_2$, the population-slope, i.e. the fixed effect of covariate SES and

##### the posterior predictive distribution of a NEW school parameter (of a school not included in the data), but from the SAME population of schools.

We sample from the "model" of the beta parameters, when $\theta$, Sigma are the current values in the chain.

This NEW parameter has been denoted by BETA.pp[,2] in the Gibbs sampler above

```{r}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,1))

plot(density(THETA.b[,2],adj=2),xlim=range(BETA.pp[,2]), 
      main="",xlab="slope parameter",ylab="posterior density",lwd=2)
lines(density(BETA.pp[,2],adj=2),col="gray",lwd=2)
legend( -3 ,1.0 ,legend=c( expression(theta[2]),expression(tilde(beta)[2])), 
        lwd=c(2,2),col=c("black","gray"),bty="n") 
```

95% posterior CI of the 'average' slope $\theta_2$

```{r}
quantile(THETA.b[,2],prob=c(.025,.5,.975))
```

very different from the prior CI that is (-3.86, 8.6)

Posterior probability that $\theta_2 >0$ is very large, but it does not indicate that any given within-school slope cannot be negative

Let us compute the probability that the slope $tilde_beta2$ (of a NEW school NOT INCLUDED in the dataset) is negative; this value is computed simulating tilde_beta2 from the slope population distribution, when parameters are the current values of $(\theta, \Sigma)$

```{r}
mean(BETA.pp[,2]<0) # this is a small value, but is is NOT equal to 0!
```

#### Posterior expectations of the 100 school -specific regression lines i.e. the lines $$y=E(\beta_{1,j}|data) + E(\beta_{2,j}|data) * x_{ij}$$

```{r}
BETA.PM<-BETA.ps/1000
plot(range(ses_cen),range(Y[,2]),type="n",xlab="SES",
   ylab="math score")
for(j in 1:m) {    abline(BETA.PM[j,1],BETA.PM[j,2],col="gray")  }
abline( mean(THETA.b[,1]),mean(THETA.b[,2]),lwd=2 )
```

COMMENT: there is a shrinkage of the 'extreme' frequentist regression lines towards the across-group average line (black line); in fact, the Bayesian estimates we got are convex linear combinations of the (unique) prior mean line and the within-group frequentist estimates.

Since we SHARED information across groups, hardly any of the slopes are negative!

```{r}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
plot( range(ses_cen),range(Y[,2]),type="n",xlab="SES", 
   ylab="math score",main="Frequentist LS within-group regression lines")
for(j in 1:m) {    abline(BETA.LS[j,1],BETA.LS[j,2],col="gray")  }

BETA.MLS<-apply(BETA.LS,2,mean)
abline(BETA.MLS[1],BETA.MLS[2],lwd=2)
plot(range(ses_cen),range(Y[,2]),type="n",xlab="SES",
   ylab="math score",main="Bayesian estimates - hierarchical model")
for(j in 1:m) {    abline(BETA.PM[j,1],BETA.PM[j,2],col="gray")  }
abline( mean(THETA.b[,1]),mean(THETA.b[,2]),lwd=2 )
```

COMMENT:

-   LEFT PLOT: 100 frequentist regression lines

-   RIGHT PLOT: there is a shrinkage of the 'extreme' frequentist regression lines towards the across-group average line (black line); in fact, the Bayesian estimates we got are convex linear combinations of the (unique) prior mean line and the within-group frequentist estimates

Since we SHARED information across groups, hardly any of the slopes are negative!
